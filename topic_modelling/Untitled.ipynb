{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhouyun/Desktop/Clean_Actors/search\n"
     ]
    }
   ],
   "source": [
    "cd /Users/zhouyun/Desktop/Clean_Actors/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_bradpitt.json         search_emorycohen.json       search_leodicaprio.json      search_michaelkeaton.json    search_stanleytucci.json\r\n",
      "search_brielarson.json       search_jacobtremblay.json    search_lievschreiber.json    search_nicholashoult.json    search_stevecarell.json\r\n",
      "search_charlizetheron.json   search_jimbroadbent.json     search_markruffalo.json      search_rachelmcadams.json    search_tomhanks.json\r\n",
      "search_christianbale.json    search_johnslattery.json     search_markrylance.json      search_ryangosling.json      search_tomhardy.json\r\n",
      "search_domhnallgleeson.json  search_juliewalters.json     search_mattdamon.json        search_saoirseronan.json     search_willpoulter.json\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,json\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import string,nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"search_bradpitt.json\") as f:\n",
    "    bradpitt_data = json.load(f)\n",
    "    \n",
    "bradpitt_text = [t['text'] for t in bradpitt_data[0]]\n",
    "bradpitt_text = ' '.join(bradpitt_text[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"search_leodicaprio.json\") as f:\n",
    "    leodicaprio_data = json.load(f)\n",
    "    \n",
    "leodicaprio_text = [t['text'] for t in leodicaprio_data[0]]\n",
    "leodicaprio_text = ' '.join(leodicaprio_text[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"search_mattdamon.json\") as f:\n",
    "    mattdamon_data = json.load(f)\n",
    "    \n",
    "mattdamon_text = [t['text'] for t in mattdamon_data[0]]\n",
    "mattdamon_text = ' '.join(mattdamon_text[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"search_tomhardy.json\") as f:\n",
    "    tomhardy_data = json.load(f)\n",
    "    \n",
    "tomhardy_text = [t['text'] for t in tomhardy_data[0]]\n",
    "tomhardy_text = ' '.join(tomhardy_text[0:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# de el en dvd\n",
    "def clean(tweet):\n",
    "    tweet = tweet.encode(\"ascii\", \"ignore\")\n",
    "    tweet = tweet.decode(\"utf-8\")\n",
    "    cleaned_words = [word.lower() for word in tweet.split() if\n",
    "                     \"http\" not in word and\n",
    "                     not word.startswith(\"@\") and\n",
    "                     not word.startswith(\".@\") and\n",
    "                     not word.startswith(\"#\") and\n",
    "                     word != \"RT\" and word != \"dvd\" and\n",
    "                     word != \"de\" and word != \"el\" and word != \"en\"]\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "def all_punct(tweet):\n",
    "    # all([c in string.punctuation for c in tweet])\n",
    "    return set(tweet).issubset(set(string.punctuation))\n",
    "\n",
    "def remove_punct(word):\n",
    "    exclude = set(string.punctuation)\n",
    "    return \"\".join(c for c in word if c not in exclude)\n",
    "\n",
    "def tokenize(tweet):\n",
    "    words = [remove_punct(w) for w in clean(tweet).split() if\n",
    "             not all_punct(w) and w not in stopwords]\n",
    "    return \" \".join(words)\n",
    "\n",
    "#tweets_list = [[tokenize(tweet) for tweet in tweets] for tweets in tweets_list]\n",
    "#tokens_list = [\" \".join(tweets) for tweets in tweets_list]\n",
    "\n",
    "#words = \" \".join(tokens_list).split()\n",
    "#vocab = sorted(set(words))\n",
    "\n",
    "def clean2(tweet):\n",
    "    cleaned_words = [word.lower() for word in tweet if\n",
    "                     word != \"tom\" and word != \"hardi\"\n",
    "                     and word != 'mad' and word != 'max' \n",
    "                     and word != 'amp']\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## 1 twitter as 1 document: too small word count\n",
    "# tomhardy_text = [t['text'] for t in tomhardy_data[0]]\n",
    "\n",
    "# tomhardy_token = [tokenize(i) for i in tomhardy_text]\n",
    "\n",
    "# tomhardy_stem = [\" \".join([p_stemmer.stem(i) for i in j.split()]) for j in tomhardy_token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# annoy_piece = {'y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label = {\n",
    "                 \"search_bradpitt.json\": {'brad','pitt','mo','e','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_brielarson.json\": {'brie','larson','e','la','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_charlizetheron.json\":{'charliz','theron','8x10','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_christianbale.json\": {'christian','bale','un','es','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_domhnallgleeson.json\":{'domhnal','gleeson','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_emorycohen.json\":{'cohen','emori','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_jacobtremblay.json\":{'jacob','tremblay','o','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_jimbroadbent.json\":{'jim','broadbent','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_johnslattery.json\":{'slatteri','john','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_juliewalters.json\":{'juli','walter','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'},\n",
    "                 \"search_leodicaprio.json\":{'dicaprio','leo','leonardo','o','se','e','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "#                  \"search_lievschreiber.json\":{'schreiber','liev','m','amp','dvd'}\n",
    "#                  \"search_markrylance.json\":{'mark','rylanc','amp','dvd'}\n",
    "#                  \"search_markruffalo.json\":{'ruffalo','mark','y','amp','dvd'}\n",
    "#                  \"search_mattdamon.json\":{'damon','matt','la','e','amp','dvd'}\n",
    "#                  \"search_michaelkeaton.json\":{'michael','keaton','amp','dvd'}\n",
    "#                  \"search_nicholashoult.json\":{'hoult','nichola','se','jd','amp','dvd'}\n",
    "#                  \"search_rachelmcadams.json\":{'mcadam','rachel','amp','dvd'}\n",
    "#                  \"search_ryangosling.json\":{'gosl','ryan','la','le','et','amp','dvd'}\n",
    "#                  \"search_saoirseronan.json\":{'saoirs','ronan','ht','amp','dvd'}\n",
    "#                  \"search_stanleytucci.json\":{'tucci','stanley','amp','dvd'}\n",
    "#                  \"search_stevecarell.json\":{'carel','steve','y','la','amp','dvd'}\n",
    "#                  \"search_tomhanks.json\":{'tom','hank','la','el','amp','dvd'}\n",
    "#                  \"search_tomhardy.json\":{'tom','hardi','amp','dvd'}\n",
    "#                  \"search_willpoulter.json\":{'poulter','un','te','amp','dvd'}\n",
    "                 }\n",
    "obvious_label[\"search_lievschreiber.json\"]={'schreiber','liev','m','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_markrylance.json\"]={'mark','rylanc','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_markruffalo.json\"]={'ruffalo','mark','y','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_mattdamon.json\"]={'damon','matt','la','e','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_michaelkeaton.json\"]={'michael','keaton','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_nicholashoult.json\"]={'hoult','nichola','se','jd','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_rachelmcadams.json\"]={'mcadam','rachel','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_ryangosling.json\"]={'gosl','ryan','la','le','et','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_saoirseronan.json\"]={'saoirs','ronan','ht','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_stanleytucci.json\"]={'tucci','stanley','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_stevecarell.json\"]={'carel','steve','y','la','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_tomhanks.json\"]={'tom','hank','la','el','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_tomhardy.json\"]={'tom','hardi','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n",
    "obvious_label[\"search_willpoulter.json\"]={'poulter','un','te','amp','dvd','y','e','la','mo','un','es','o','se','m','se','et','le','el','te'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeStdDate(tomhardy_data,js):\n",
    "    tomhardy_text = [t['text'] for t in tomhardy_data[0]]\n",
    "    tomhardy_token = [tokenize(i) for i in tomhardy_text]\n",
    "    ## combine 100 twitters as 1 document \n",
    "    a = []\n",
    "    i=0\n",
    "    while i < len(tomhardy_token):\n",
    "        temp = ' '.join(tomhardy_token[i:min(i+100,len(tomhardy_token))])\n",
    "        a.append(temp)\n",
    "        i += 100\n",
    "\n",
    "    tomhardy_token = a\n",
    "    tomhardy_stem = [[p_stemmer.stem(i) for i in j.split()] for j in tomhardy_token]\n",
    "    \n",
    "    tomhardy_stem = [clean2(i,js) for i in tomhardy_stem]\n",
    "    \n",
    "    tomhardy_dict = corpora.Dictionary(tomhardy_stem)\n",
    "    tomhardy_corpus = [tomhardy_dict.doc2bow(i) for i in tomhardy_stem]\n",
    "    \n",
    "    return tomhardy_corpus,tomhardy_dict\n",
    "\n",
    "## get rid of Actor name and Movie name\n",
    "\n",
    "def clean2(tweet,js):\n",
    "    cleaned_words = [word.lower() for word in tweet if\n",
    "                     word not in obvious_label[js]]\n",
    "    return cleaned_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_bradpitt.json\n",
      "search_brielarson.json\n",
      "search_charlizetheron.json\n",
      "search_christianbale.json\n",
      "search_domhnallgleeson.json\n",
      "search_emorycohen.json\n",
      "search_jacobtremblay.json\n",
      "search_jimbroadbent.json\n",
      "search_johnslattery.json\n",
      "search_juliewalters.json\n",
      "search_leodicaprio.json\n",
      "search_lievschreiber.json\n",
      "search_markruffalo.json\n",
      "search_markrylance.json\n",
      "search_mattdamon.json\n",
      "search_michaelkeaton.json\n",
      "search_nicholashoult.json\n",
      "search_rachelmcadams.json\n",
      "search_ryangosling.json\n",
      "search_saoirseronan.json\n",
      "search_stanleytucci.json\n",
      "search_stevecarell.json\n",
      "search_tomhanks.json\n",
      "search_tomhardy.json\n",
      "search_willpoulter.json\n"
     ]
    }
   ],
   "source": [
    "path_to_json = '/Users/zhouyun/Desktop/Clean_Actors/search/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "all_topic4 = {}\n",
    "for js in json_files:\n",
    "    with open(path_to_json+js) as infile:\n",
    "        actor_data = json.load(infile)\n",
    "    \n",
    "    actor_corpus, actor_dict = MakeStdDate(actor_data,js)\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(actor_corpus, num_topics=4, \n",
    "                                           id2word = actor_dict, passes=10, eval_every = 5,minimum_probability = 0.02)\n",
    "    all_topic4[js] = ldamodel.print_topics()\n",
    "    print(js)\n",
    "#1 3 11 24 21 4\n",
    "#0 2 10 23 20 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_topic4.json','w') as f:\n",
    "    json.dump(all_topic4,f,indent=4,sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['search_leodicaprio.json', 'search_jimbroadbent.json', 'search_brielarson.json', 'search_ryangosling.json', 'search_emorycohen.json', 'search_rachelmcadams.json', 'search_lievschreiber.json', 'search_nicholashoult.json', 'search_domhnallgleeson.json', 'search_mattdamon.json', 'search_saoirseronan.json', 'search_christianbale.json', 'search_bradpitt.json', 'search_michaelkeaton.json', 'search_willpoulter.json', 'search_juliewalters.json', 'search_stevecarell.json', 'search_johnslattery.json', 'search_tomhardy.json', 'search_jacobtremblay.json', 'search_markrylance.json', 'search_markruffalo.json', 'search_charlizetheron.json', 'search_stanleytucci.json', 'search_tomhanks.json'])"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topic4.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_topic3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_bradpitt.json': [(0,\n",
       "   '0.045*angelina + 0.043*joli + 0.023*mr + 0.015*8x10 + 0.014*new + 0.012*weight + 0.011*bluray + 0.010*widescreen + 0.010*smith + 0.007*sign'),\n",
       "  (1,\n",
       "   '0.030*angelina + 0.030*joli + 0.022*8x10 + 0.016*new + 0.014*weight + 0.012*sign + 0.010*bluray + 0.010*widescreen + 0.009*set + 0.009*fight'),\n",
       "  (2,\n",
       "   '0.021*sooyoung + 0.021*mask + 0.021*hold + 0.021*lmfao + 0.015*angelina + 0.011*que + 0.010*look + 0.010*loui + 0.010*dada + 0.009*joli'),\n",
       "  (3,\n",
       "   '0.038*dada + 0.038*loui + 0.034*angelina + 0.019*ma + 0.019*daniel + 0.019*amor + 0.019*amizad + 0.019*ano + 0.019*casal + 0.019*harri')],\n",
       " 'search_brielarson.json': [(0,\n",
       "   '0.028*award + 0.022*2016 + 0.020*oscar + 0.020*sag + 0.019*room + 0.018*tremblay + 0.017*jacob + 0.017*ador + 0.016*best + 0.016*star'),\n",
       "  (1,\n",
       "   '0.059*vaniti + 0.059*fair + 0.054*2016 + 0.049*vikand + 0.049*saoirs + 0.049*alicia + 0.049*ronan + 0.049*nyongo + 0.049*lupita + 0.048*anni'),\n",
       "  (2,\n",
       "   '0.015*award + 0.014*room + 0.013*sag + 0.012*jacob + 0.011*tremblay + 0.011*2016 + 0.011*role + 0.010*oscar + 0.010*eu + 0.010*best'),\n",
       "  (3,\n",
       "   '0.023*room + 0.017*award + 0.016*sag + 0.012*oscar + 0.011*2016 + 0.009*best + 0.008*actress + 0.007*film + 0.006*win + 0.006*tremblay')]}"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_topic2.json','w') as f:\n",
    "    json.dump(all_topic2,f,indent=4,sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.044*angelina + 0.043*joli + 0.025*8x10 + 0.018*mr + 0.015*weight + 0.014*sign + 0.010*new + 0.010*widescreen + 0.010*bluray + 0.009*pictur'"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topic2['search_bradpitt.json'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0.044', 'angelina'],\n",
       " ['0.043', 'joli'],\n",
       " ['0.025', '8x10'],\n",
       " ['0.018', 'mr'],\n",
       " ['0.015', 'weight'],\n",
       " ['0.014', 'sign'],\n",
       " ['0.010', 'new'],\n",
       " ['0.010', 'widescreen'],\n",
       " ['0.010', 'bluray'],\n",
       " ['0.009', 'pictur'],\n",
       " ['0.024', 'joli'],\n",
       " ['0.024', 'angelina'],\n",
       " ['0.020', 'new'],\n",
       " ['0.015', '8x10'],\n",
       " ['0.013', 'weight'],\n",
       " ['0.011', 'bluray'],\n",
       " ['0.011', 'set'],\n",
       " ['0.010', 'widescreen'],\n",
       " ['0.008', 'divorc'],\n",
       " ['0.008', 'loss'],\n",
       " ['0.045', 'loui'],\n",
       " ['0.045', 'dada'],\n",
       " ['0.034', 'angelina'],\n",
       " ['0.024', 'ma'],\n",
       " ['0.023', 'hold'],\n",
       " ['0.022', 'sooyoung'],\n",
       " ['0.022', 'lmfao'],\n",
       " ['0.022', 'mask'],\n",
       " ['0.022', 'amor'],\n",
       " ['0.022', 'daniel'],\n",
       " ['0.016', 'angelina'],\n",
       " ['0.015', 'joli'],\n",
       " ['0.012', 'que'],\n",
       " ['0.011', 'look'],\n",
       " ['0.007', 'like'],\n",
       " ['0.006', '18'],\n",
       " ['0.006', 'con'],\n",
       " ['0.006', 'get'],\n",
       " ['0.006', 'decemb'],\n",
       " ['0.006', '1963']]"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([[i.split('*') for i in j[1].split(' + ')] for j in all_topic2['search_bradpitt.json']],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_topic3 = [[[i.split('*') for i in j[1].split(' + ')] for j in all_topic2[k]] for k in sorted(all_topic2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brad_pitt = [i[:5] for i in all_topic3[0]]\n",
    "charlize_theron = [i[:5] for i in all_topic3[2]]\n",
    "leo_dicaprio = [i[:5] for i in all_topic3[10]]\n",
    "tom_hardy = [i[:5] for i in all_topic3[23]]\n",
    "stanley_tucci = [i[:5] for i in all_topic3[20]]\n",
    "christian_bale = [i[:5] for i in all_topic3[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['0.044', 'angelina'],\n",
       "  ['0.043', 'joli'],\n",
       "  ['0.025', '8x10'],\n",
       "  ['0.018', 'mr'],\n",
       "  ['0.015', 'weight']],\n",
       " [['0.024', 'joli'],\n",
       "  ['0.024', 'angelina'],\n",
       "  ['0.020', 'new'],\n",
       "  ['0.015', '8x10'],\n",
       "  ['0.013', 'weight']],\n",
       " [['0.045', 'loui'],\n",
       "  ['0.045', 'dada'],\n",
       "  ['0.034', 'angelina'],\n",
       "  ['0.024', 'ma'],\n",
       "  ['0.023', 'hold']],\n",
       " [['0.016', 'angelina'],\n",
       "  ['0.015', 'joli'],\n",
       "  ['0.012', 'que'],\n",
       "  ['0.011', 'look'],\n",
       "  ['0.007', 'like']]]"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brad_pitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brad_pitt_freq = [i[0] for i in brad_pitt[0]] + [i[0] for i in brad_pitt[1]] + [i[0] for i in brad_pitt[2]] + [i[0] for i in brad_pitt[3]]\n",
    "brad_pitt_domain = [i[1] for i in brad_pitt[0]] + [i[1] for i in brad_pitt[1]] + [i[1] for i in brad_pitt[2]] + [i[1] for i in brad_pitt[3]]\n",
    "brad_pitt_df = pd.DataFrame(brad_pitt_domain,brad_pitt_freq)\n",
    "brad_pitt_df.to_csv ('bp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for kkk in ['brad_pitt','charlize_theron','leo_dicaprio','tom_hardy','stanley_tucci','christian_bale']:\n",
    "    actor_name = eval(kkk)\n",
    "    brad_pitt_freq = [i[0] for i in actor_name[0]] + [i[0] for i in actor_name[1]] + [i[0] for i in actor_name[2]] + [i[0] for i in actor_name[3]]\n",
    "    brad_pitt_domain = [i[1] for i in actor_name[0]] + [i[1] for i in actor_name[1]] + [i[1] for i in actor_name[2]] + [i[1] for i in actor_name[3]]\n",
    "    brad_pitt_df = pd.DataFrame(brad_pitt_domain,brad_pitt_freq)\n",
    "    brad_pitt_df.to_csv(kkk + '.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-keyword arg after keyword arg (<ipython-input-670-860834e44266>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-670-860834e44266>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ggplot(data = brad_pitt_df, aes(x='brad_pitt_domain',y='brad_pitt_freq'))\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-keyword arg after keyword arg\n"
     ]
    }
   ],
   "source": [
    "# ggplot(data = brad_pitt_df, aes(x='brad_pitt_domain',y='brad_pitt_freq')) +\\\n",
    "#           geom_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ggplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('qimingzi.json','w') as f:\n",
    "    json.dump(brad_pitt,f,indent=4,sort_keys=True)\n",
    "    json.dump(charlize_theron,f,indent=4,sort_keys=True)\n",
    "    json.dump(leo_dicaprio,f,indent=4,sort_keys=True)\n",
    "    json.dump(tom_hardy,f,indent=4,sort_keys=True)\n",
    "    json.dump(stanley_tucci,f,indent=4,sort_keys=True)\n",
    "    json.dump(christian_bale,f,indent=4,sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_topic3.json', 'w') as f:\n",
    "    json.dump(all_topic3,f,indent=4,sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_bradpitt.json': [(0,\n",
       "   '0.037*angelina + 0.035*joli + 0.022*8x10 + 0.016*new + 0.015*weight + 0.014*mr + 0.012*sign + 0.011*bluray + 0.011*widescreen + 0.009*pictur'),\n",
       "  (1,\n",
       "   '0.040*mo + 0.040*loui + 0.040*dada + 0.034*angelina + 0.021*ma + 0.020*harri + 0.020*hold + 0.020*lmfao + 0.020*mask + 0.020*sooyoung'),\n",
       "  (2,\n",
       "   '0.015*angelina + 0.014*joli + 0.011*que + 0.011*look + 0.010*la + 0.009*y + 0.008*like + 0.007*o + 0.006*un + 0.006*con'),\n",
       "  (3,\n",
       "   '0.019*exclus + 0.019*divorc + 0.018*head + 0.014*follow + 0.010*pleas + 0.009*y + 0.006*club + 0.006*fight + 0.005*mr + 0.004*keep')],\n",
       " 'search_brielarson.json': [(0,\n",
       "   '0.065*fair + 0.065*vaniti + 0.058*2016 + 0.052*vikand + 0.052*alicia + 0.052*lupita + 0.052*nyongo + 0.052*ronan + 0.052*saoirs + 0.051*anni'),\n",
       "  (1,\n",
       "   '0.042*2016 + 0.041*vikand + 0.041*alicia + 0.041*saoirs + 0.040*lupita + 0.040*leibovitz + 0.040*anni + 0.040*nyongo + 0.040*ronan + 0.040*fair'),\n",
       "  (2,\n",
       "   '0.053*oscar + 0.038*hall + 0.038*tanner + 0.037*rooney + 0.037*mara + 0.037*togeth + 0.036*2009 + 0.036*nomine + 0.036*star + 0.013*5'),\n",
       "  (3,\n",
       "   '0.024*room + 0.019*award + 0.016*best + 0.015*sag + 0.014*role + 0.014*oscar + 0.011*2016 + 0.010*appreci + 0.009*tremblay + 0.009*jacob')]}"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_bradpitt.json': [(0,\n",
       "   '0.030*mask + 0.030*sooyoung + 0.030*hold + 0.030*lmfao + 0.015*joli + 0.014*angelina + 0.009*get + 0.007*new + 0.007*world + 0.007*dont'),\n",
       "  (1,\n",
       "   '0.036*angelina + 0.035*joli + 0.022*8x10 + 0.016*new + 0.015*weight + 0.014*mr + 0.011*sign + 0.011*bluray + 0.011*widescreen + 0.009*loss'),\n",
       "  (2,\n",
       "   '0.011*angelina + 0.011*joli + 0.011*look + 0.010*y + 0.010*que + 0.010*la + 0.007*like + 0.007*18 + 0.006*decemb + 0.006*1963'),\n",
       "  (3,\n",
       "   '0.039*loui + 0.039*mo + 0.039*dada + 0.034*angelina + 0.020*ma + 0.020*amor + 0.020*ano + 0.019*daniel + 0.019*harri + 0.019*casal')],\n",
       " 'search_brielarson.json': [(0,\n",
       "   '0.022*2016 + 0.020*oscar + 0.016*room + 0.015*fair + 0.015*vaniti + 0.015*vikand + 0.015*alicia + 0.014*saoirs + 0.014*ronan + 0.014*anni'),\n",
       "  (1,\n",
       "   '0.059*vaniti + 0.059*fair + 0.054*2016 + 0.050*vikand + 0.050*alicia + 0.049*saoirs + 0.049*nyongo + 0.049*ronan + 0.049*lupita + 0.048*leibovitz'),\n",
       "  (2,\n",
       "   '0.023*room + 0.022*best + 0.021*award + 0.019*role + 0.014*appreci + 0.014*oscar + 0.013*sag + 0.009*preroommovi + 0.008*tremblay + 0.008*actress'),\n",
       "  (3,\n",
       "   '0.018*room + 0.014*sag + 0.014*award + 0.013*2016 + 0.010*que + 0.010*oscar + 0.009*win + 0.009*lo + 0.009*actress + 0.008*role')]}"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topic2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"search_bradpitt.json\") as f:\n",
    "    bradpitt_data = json.load(f)\n",
    "    \n",
    "bradpitt_corpus = MakeStdDate(bradpitt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wed Feb 03 03:38:48 +0000 2016',\n",
       " 'Wed Feb 03 03:12:44 +0000 2016',\n",
       " 'Wed Feb 03 03:10:09 +0000 2016',\n",
       " 'Wed Feb 03 03:05:20 +0000 2016',\n",
       " 'Wed Feb 03 03:02:17 +0000 2016',\n",
       " 'Wed Feb 03 03:02:07 +0000 2016',\n",
       " 'Wed Feb 03 02:59:55 +0000 2016',\n",
       " 'Wed Feb 03 02:47:53 +0000 2016',\n",
       " 'Wed Feb 03 02:27:33 +0000 2016',\n",
       " 'Wed Feb 03 02:20:16 +0000 2016',\n",
       " 'Wed Feb 03 02:12:55 +0000 2016',\n",
       " 'Wed Feb 03 02:11:32 +0000 2016',\n",
       " 'Wed Feb 03 01:26:59 +0000 2016',\n",
       " 'Wed Feb 03 01:11:48 +0000 2016',\n",
       " 'Wed Feb 03 01:08:51 +0000 2016',\n",
       " 'Wed Feb 03 01:03:41 +0000 2016',\n",
       " 'Wed Feb 03 01:02:20 +0000 2016',\n",
       " 'Wed Feb 03 00:57:24 +0000 2016',\n",
       " 'Wed Feb 03 00:57:13 +0000 2016',\n",
       " 'Wed Feb 03 00:55:38 +0000 2016',\n",
       " 'Wed Feb 03 00:55:30 +0000 2016',\n",
       " 'Wed Feb 03 00:54:43 +0000 2016',\n",
       " 'Wed Feb 03 00:54:35 +0000 2016',\n",
       " 'Wed Feb 03 00:54:29 +0000 2016',\n",
       " 'Wed Feb 03 00:54:20 +0000 2016',\n",
       " 'Wed Feb 03 00:47:02 +0000 2016',\n",
       " 'Wed Feb 03 00:22:43 +0000 2016',\n",
       " 'Wed Feb 03 00:17:26 +0000 2016',\n",
       " 'Wed Feb 03 00:11:37 +0000 2016',\n",
       " 'Wed Feb 03 00:08:56 +0000 2016',\n",
       " 'Wed Feb 03 00:08:44 +0000 2016',\n",
       " 'Wed Feb 03 00:08:02 +0000 2016',\n",
       " 'Wed Feb 03 00:06:45 +0000 2016',\n",
       " 'Wed Feb 03 00:06:14 +0000 2016',\n",
       " 'Wed Feb 03 00:04:09 +0000 2016',\n",
       " 'Wed Feb 03 00:00:06 +0000 2016',\n",
       " 'Tue Feb 02 23:50:25 +0000 2016',\n",
       " 'Tue Feb 02 23:41:53 +0000 2016',\n",
       " 'Tue Feb 02 23:38:22 +0000 2016',\n",
       " 'Tue Feb 02 23:37:15 +0000 2016',\n",
       " 'Tue Feb 02 23:37:15 +0000 2016',\n",
       " 'Tue Feb 02 23:37:15 +0000 2016',\n",
       " 'Tue Feb 02 23:37:15 +0000 2016',\n",
       " 'Tue Feb 02 23:27:16 +0000 2016',\n",
       " 'Tue Feb 02 23:18:50 +0000 2016',\n",
       " 'Tue Feb 02 23:08:35 +0000 2016',\n",
       " 'Tue Feb 02 23:01:04 +0000 2016',\n",
       " 'Tue Feb 02 22:43:07 +0000 2016',\n",
       " 'Tue Feb 02 22:42:40 +0000 2016',\n",
       " 'Tue Feb 02 22:35:06 +0000 2016',\n",
       " 'Tue Feb 02 22:28:09 +0000 2016',\n",
       " 'Tue Feb 02 22:26:17 +0000 2016',\n",
       " 'Tue Feb 02 22:18:23 +0000 2016',\n",
       " 'Tue Feb 02 22:16:33 +0000 2016',\n",
       " 'Tue Feb 02 22:05:55 +0000 2016',\n",
       " 'Tue Feb 02 21:59:42 +0000 2016',\n",
       " 'Tue Feb 02 21:59:09 +0000 2016',\n",
       " 'Tue Feb 02 21:59:00 +0000 2016',\n",
       " 'Tue Feb 02 21:58:25 +0000 2016',\n",
       " 'Tue Feb 02 21:58:24 +0000 2016',\n",
       " 'Tue Feb 02 21:58:24 +0000 2016',\n",
       " 'Tue Feb 02 21:58:24 +0000 2016',\n",
       " 'Tue Feb 02 21:58:24 +0000 2016',\n",
       " 'Tue Feb 02 21:57:58 +0000 2016',\n",
       " 'Tue Feb 02 21:53:27 +0000 2016',\n",
       " 'Tue Feb 02 21:48:24 +0000 2016',\n",
       " 'Tue Feb 02 21:48:08 +0000 2016',\n",
       " 'Tue Feb 02 21:47:47 +0000 2016',\n",
       " 'Tue Feb 02 21:46:22 +0000 2016',\n",
       " 'Tue Feb 02 21:32:57 +0000 2016',\n",
       " 'Tue Feb 02 21:27:58 +0000 2016',\n",
       " 'Tue Feb 02 21:25:28 +0000 2016',\n",
       " 'Tue Feb 02 21:21:38 +0000 2016',\n",
       " 'Tue Feb 02 21:19:53 +0000 2016',\n",
       " 'Tue Feb 02 21:16:36 +0000 2016',\n",
       " 'Tue Feb 02 21:14:51 +0000 2016',\n",
       " 'Tue Feb 02 21:05:01 +0000 2016',\n",
       " 'Tue Feb 02 21:04:52 +0000 2016',\n",
       " 'Tue Feb 02 21:03:44 +0000 2016',\n",
       " 'Tue Feb 02 21:01:07 +0000 2016',\n",
       " 'Tue Feb 02 20:51:48 +0000 2016',\n",
       " 'Tue Feb 02 20:48:20 +0000 2016',\n",
       " 'Tue Feb 02 20:30:02 +0000 2016',\n",
       " 'Tue Feb 02 20:22:55 +0000 2016',\n",
       " 'Tue Feb 02 20:22:20 +0000 2016',\n",
       " 'Tue Feb 02 20:17:03 +0000 2016',\n",
       " 'Tue Feb 02 20:17:03 +0000 2016',\n",
       " 'Tue Feb 02 20:17:02 +0000 2016',\n",
       " 'Tue Feb 02 20:12:02 +0000 2016',\n",
       " 'Tue Feb 02 20:05:52 +0000 2016',\n",
       " 'Tue Feb 02 20:05:15 +0000 2016',\n",
       " 'Tue Feb 02 19:58:24 +0000 2016',\n",
       " 'Tue Feb 02 19:58:12 +0000 2016',\n",
       " 'Tue Feb 02 19:57:59 +0000 2016',\n",
       " 'Tue Feb 02 19:57:18 +0000 2016',\n",
       " 'Tue Feb 02 19:56:13 +0000 2016',\n",
       " 'Tue Feb 02 19:46:28 +0000 2016',\n",
       " 'Tue Feb 02 19:43:03 +0000 2016',\n",
       " 'Tue Feb 02 19:34:12 +0000 2016',\n",
       " 'Tue Feb 02 19:32:08 +0000 2016']"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tomhardy_text = [t['text'] for t in tomhardy_data[0]]\n",
    "\n",
    "tomhardy_token = [tokenize(i) for i in tomhardy_text]\n",
    "\n",
    "## combine 100 twitters as 1 document \n",
    "a = []\n",
    "i=0\n",
    "\n",
    "while i < len(tomhardy_token):\n",
    "    temp = ' '.join(tomhardy_token[i:min(i+100,len(tomhardy_token))])\n",
    "    a.append(temp)\n",
    "    i += 100\n",
    "                                   \n",
    "tomhardy_token = a\n",
    "\n",
    "tomhardy_stem = [[p_stemmer.stem(i) for i in j.split()] for j in tomhardy_token]\n",
    "\n",
    "## get rid of Actor name and Movie name\n",
    "tomhardy_stem = [clean2(i) for i in tomhardy_stem]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tomhardy_dict = corpora.Dictionary(tomhardy_stem)\n",
    "\n",
    "tomhardy_corpus = [tomhardy_dict.doc2bow(i) for i in tomhardy_stem]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(tomhardy_corpus, num_topics=3, \n",
    "                                           id2word = tomhardy_dict, passes=60)\n",
    "ldamodel.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_bradpitt.json\n",
      "search_brielarson.json\n",
      "search_charlizetheron.json\n",
      "search_christianbale.json\n",
      "search_domhnallgleeson.json\n",
      "search_emorycohen.json\n",
      "search_jacobtremblay.json\n",
      "search_jimbroadbent.json\n",
      "search_johnslattery.json\n",
      "search_juliewalters.json\n",
      "search_leodicaprio.json\n",
      "search_lievschreiber.json\n",
      "search_markruffalo.json\n",
      "search_markrylance.json\n",
      "search_mattdamon.json\n",
      "search_michaelkeaton.json\n",
      "search_nicholashoult.json\n",
      "search_rachelmcadams.json\n",
      "search_ryangosling.json\n",
      "search_saoirseronan.json\n",
      "search_stanleytucci.json\n",
      "search_stevecarell.json\n",
      "search_tomhanks.json\n",
      "search_tomhardy.json\n",
      "search_willpoulter.json\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p_stemmer = PorterStemmer()\n",
    "# all_stem = []\n",
    "\n",
    "# tomhardy_stem = [p_stemmer.stem(i) for i in tomhardy_token.split()]\n",
    "# all_stem.append(tomhardy_stem)\n",
    "\n",
    "# mattdamon_stem = [p_stemmer.stem(i) for i in mattdamon_token.split()]\n",
    "# all_stem.append(mattdamon_stem)\n",
    "\n",
    "# leodicaprio_stem = [p_stemmer.stem(i) for i in leodicaprio_token.split()]\n",
    "# all_stem.append(leodicaprio_stem)\n",
    "\n",
    "# bradpitt_stem = [p_stemmer.stem(i) for i in bradpitt_token.split()]\n",
    "# all_stem.append(bradpitt_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*ford + 0.018*men + 0.018*week + 0.017*david + 0.017*dress + 0.017*feat + 0.017*beckham + 0.013*film + 0.013*watch + 0.013*legend'),\n",
       " (1,\n",
       "  '0.020*legend + 0.017*film + 0.013*reven + 0.011*one + 0.010*watch + 0.009*oscar + 0.009*perform + 0.008*que + 0.007*two + 0.007*dicaprio'),\n",
       " (2,\n",
       "  '0.134*legend + 0.131*dog + 0.130*premier + 0.129*took + 0.129*rememb + 0.018*workout + 0.004*film + 0.003*reven + 0.002*evolut + 0.002*o'),\n",
       " (3,\n",
       "  '0.041*the + 0.039*problem + 0.039*thing + 0.038*open + 0.038*addict + 0.038*past + 0.038*save + 0.032*th + 0.009*dark + 0.008*time')]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(tomhardy_corpus, num_topics=4, \n",
    "                                           id2word = tomhardy_dict, passes=60)\n",
    "## without Actor/Movie name, 100 twitter as 1 document\n",
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*legend + 0.017*film + 0.012*reven + 0.012*watch + 0.009*one + 0.009*actor + 0.008*oscar + 0.008*que + 0.007*perform + 0.007*movi'),\n",
       " (1,\n",
       "  '0.128*legend + 0.126*dog + 0.124*rememb + 0.124*took + 0.124*premier + 0.017*workout + 0.004*film + 0.003*reven + 0.002*o + 0.002*evolut'),\n",
       " (2,\n",
       "  '0.033*the + 0.033*ford + 0.032*problem + 0.032*david + 0.032*week + 0.032*men + 0.032*thing + 0.031*dress + 0.031*feat + 0.031*addict')]"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(tomhardy_corpus, num_topics=7, \n",
    "                                           id2word = tomhardy_dict, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.040*hardi + 0.039*tom + 0.028*bless + 0.027*timelin + 0.026*ft + 0.026*situat + 0.026*puppi + 0.025*beauti + 0.006*dicaprio + 0.006*leo'),\n",
       " (1,\n",
       "  '0.139*tom + 0.139*hardi + 0.093*legend + 0.087*dog + 0.085*rememb + 0.085*took + 0.085*premier + 0.011*workout + 0.003*film + 0.002*reven'),\n",
       " (2,\n",
       "  '0.069*hardi + 0.069*tom + 0.047*cgi + 0.046*oldschool + 0.046*trickeri + 0.046*two + 0.044*tower + 0.044*one + 0.044*creat + 0.042*perform'),\n",
       " (3,\n",
       "  '0.106*tom + 0.104*hardi + 0.013*legend + 0.012*reven + 0.010*film + 0.010*watch + 0.008*que + 0.008*oscar + 0.007*actor + 0.006*like'),\n",
       " (4,\n",
       "  '0.020*legend + 0.019*film + 0.019*review + 0.011*actor + 0.010*latest + 0.009*modern + 0.008*watch + 0.008*18 + 0.008*site + 0.008*british'),\n",
       " (5,\n",
       "  '0.058*tom + 0.033*hardi + 0.030*amp + 0.025*men + 0.024*week + 0.024*ford + 0.024*david + 0.024*feat + 0.024*beckham + 0.024*dress'),\n",
       " (6,\n",
       "  '0.067*hardi + 0.066*tom + 0.064*problem + 0.064*addict + 0.063*thing + 0.063*open + 0.063*past + 0.063*save + 0.062*the + 0.053*th')]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##有人名 tom hardy 电影名， 100个twitter一文章\n",
    "## 画图 ： 有/无 人名电影名， 100twitter 一文章\n",
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.040*hardi + 0.039*tom + 0.028*bless'),\n",
       " (1, '0.139*tom + 0.139*hardi + 0.093*legend'),\n",
       " (2, '0.069*hardi + 0.069*tom + 0.047*cgi'),\n",
       " (3, '0.106*tom + 0.104*hardi + 0.013*legend'),\n",
       " (4, '0.020*legend + 0.019*film + 0.019*review'),\n",
       " (5, '0.058*tom + 0.033*hardi + 0.030*amp'),\n",
       " (6, '0.067*hardi + 0.066*tom + 0.064*problem')]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=10, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(tomhardy_corpus, num_topics=7, \n",
    "                                           id2word = tomhardy_dict, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.081*tom + 0.036*amp + 0.031*dress + 0.031*feat + 0.031*beckham + 0.026*beauti + 0.022*star + 0.020*timelin + 0.020*bless + 0.019*mad'),\n",
       " (1,\n",
       "  '0.103*hardi + 0.097*tom + 0.026*film + 0.019*leo + 0.019*oscar + 0.019*one + 0.017*reven + 0.016*two + 0.016*perform + 0.013*o'),\n",
       " (2,\n",
       "  '0.129*hardi + 0.121*tom + 0.028*watch + 0.017*week + 0.016*ford + 0.016*film + 0.015*like + 0.015*reven + 0.013*movi + 0.013*kray'),\n",
       " (3,\n",
       "  '0.066*tom + 0.066*hardi + 0.025*que + 0.024*la + 0.021*win + 0.020*lo + 0.017*y + 0.016*es + 0.015*un + 0.013*oscar'),\n",
       " (4,\n",
       "  '0.082*tom + 0.080*hardi + 0.054*men + 0.037*thing + 0.031*problem + 0.031*addict + 0.029*open + 0.029*save + 0.026*th + 0.021*want'),\n",
       " (5,\n",
       "  '0.059*the + 0.038*past + 0.019*si + 0.012*road + 0.011*3 + 0.010*mad + 0.009*max + 0.009*furi + 0.009*ver + 0.009*1'),\n",
       " (6,\n",
       "  '0.183*hardi + 0.174*tom + 0.133*legend + 0.087*dog + 0.080*rememb + 0.080*premier + 0.080*took + 0.018*david + 0.011*workout + 0.003*well')]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##有人名 tom hardy 电影名， 一个twitter一文章\n",
    "## 画图 ： 有/无 人名电影名， 100twitter 一文章\n",
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.081*tom + 0.036*amp + 0.031*dress'),\n",
       " (1, '0.103*hardi + 0.097*tom + 0.026*film'),\n",
       " (2, '0.129*hardi + 0.121*tom + 0.028*watch'),\n",
       " (3, '0.066*tom + 0.066*hardi + 0.025*que'),\n",
       " (4, '0.082*tom + 0.080*hardi + 0.054*men'),\n",
       " (5, '0.059*the + 0.038*past + 0.019*si'),\n",
       " (6, '0.183*hardi + 0.174*tom + 0.133*legend')]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=10, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.000*hardi + 0.000*tom + 0.000*brad'),\n",
       " (1, '0.000*damon + 0.000*matt + 0.000*hardi'),\n",
       " (2, '0.088*matt + 0.088*damon + 0.008*martian'),\n",
       " (3, '0.090*dicaprio + 0.076*leonardo + 0.036*kate'),\n",
       " (4, '0.039*brad + 0.039*pitt + 0.025*dvd'),\n",
       " (5, '0.000*dicaprio + 0.000*brad + 0.000*pitt'),\n",
       " (6, '0.090*tom + 0.089*hardi + 0.053*legend')]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=10, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.000*hardi + 0.000*tom + 0.000*brad + 0.000*pitt + 0.000*rememb + 0.000*took + 0.000*dvd + 0.000*legend + 0.000*premier + 0.000*leonardo'),\n",
       " (1,\n",
       "  '0.000*damon + 0.000*matt + 0.000*hardi + 0.000*dicaprio + 0.000*leonardo + 0.000*tom + 0.000*brad + 0.000*took + 0.000*de + 0.000*dvd'),\n",
       " (2,\n",
       "  '0.088*matt + 0.088*damon + 0.008*martian + 0.007*de + 0.007*bourn + 0.006*movi + 0.005*en + 0.005*dvd + 0.005*film + 0.005*la'),\n",
       " (3,\n",
       "  '0.090*dicaprio + 0.076*leonardo + 0.036*kate + 0.036*winslet + 0.031*now + 0.018*leo + 0.016*oscar + 0.012*de + 0.011*el + 0.010*que'),\n",
       " (4,\n",
       "  '0.039*brad + 0.039*pitt + 0.025*dvd + 0.022*angelina + 0.022*joli + 0.017*รอบนี้มากับสุดที่รักอย่างเฮียแบรด + 0.017*คุณแม่แองจี้ + 0.017*บินมาสักที่ไทยอีกแล้ว + 0.017*หลงไหลลายสักมาก + 0.012*8x10'),\n",
       " (5,\n",
       "  '0.000*dicaprio + 0.000*brad + 0.000*pitt + 0.000*matt + 0.000*leonardo + 0.000*damon + 0.000*dvd + 0.000*angelina + 0.000*joli + 0.000*คุณแม่แองจี้'),\n",
       " (6,\n",
       "  '0.090*tom + 0.089*hardi + 0.053*legend + 0.047*dog + 0.047*premier + 0.047*took + 0.047*rememb + 0.008*film + 0.007*workout + 0.006*reven')]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
